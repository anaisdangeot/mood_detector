{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "      <td>aint ever trap bando oh lord dont get wrong kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "      <td>drink go smoke go feel get let go care get los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "      <td>dont live planet earth find love venus thats w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "      <td>trippin grigio mobbin light low trippin grigio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "      <td>see midnight panther gallant brave find find a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0        artist  \\\n",
       "0             0           0  Elijah Blake   \n",
       "1             1           1  Elijah Blake   \n",
       "2             2           2  Elijah Blake   \n",
       "3             3           3  Elijah Blake   \n",
       "4             4           4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  No, no\\r\\nI ain't ever trapped out the bando\\r...            Everyday   \n",
       "1  The drinks go down and smoke goes up, I feel m...    Live Till We Die   \n",
       "2  She don't live on planet Earth no more\\r\\nShe ...       The Otherside   \n",
       "3  Trippin' off that Grigio, mobbin', lights low\\...               Pinot   \n",
       "4  I see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "\n",
       "   label                                     cleaned_lyrics  \n",
       "0  0.626  aint ever trap bando oh lord dont get wrong kn...  \n",
       "1  0.630  drink go smoke go feel get let go care get los...  \n",
       "2  0.240  dont live planet earth find love venus thats w...  \n",
       "3  0.536  trippin grigio mobbin light low trippin grigio...  \n",
       "4  0.371  see midnight panther gallant brave find find a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/labeled_lyrics_cleaned_processed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_lyrics'].isna().sum()\n",
    "data['cleaned_lyrics'] = data['cleaned_lyrics'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = data.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5190\n",
       "1    4810\n",
       "Name: mood, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cat_valence(row):\n",
    "    if row >= 0.5:\n",
    "        return 1\n",
    "    elif row <0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "data_sub['mood'] = data_sub['label'].apply(lambda x:cat_valence(x))\n",
    "data_sub['mood'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Feature/Target\n",
    "X = data_sub['cleaned_lyrics'].apply(lambda x: np.str_(x))\n",
    "y = data_sub[\"mood\"]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline using logistic regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv = 5, scoring = [\"accuracy\"])\n",
    "average_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "baseline_accuracy = np.round(average_accuracy,2)\n",
    "baseline_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'tfidfvectorizer__ngram_range': ((2,2), (1,2)),\n",
    "    'tfidfvectorizer__max_df': [0,25, 0.3, 0.35],\n",
    "    'tfidfvectorizer__max_features': [50],\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    parameters,\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After different iterations, best params: max_df= 0.35, max_features=50, n_gram=(1,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.35,max_features=50)\n",
    "\n",
    "vectorized_documents = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-15 07:44:39.877275: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 07:44:40.258316: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-15 07:44:40.261387: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-15 07:44:43.269326: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26275 different words in your corpus\n",
      "X_pad.shape (7000, 637)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.680e+02, 2.380e+02, 4.200e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [7.620e+03, 8.977e+03, 8.978e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [3.500e+01, 1.000e+00, 5.400e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [3.910e+02, 3.910e+02, 5.400e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [5.500e+01, 5.200e+01, 5.000e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.300e+01, 7.300e+01, 4.200e+01, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "### Let's create some mock data\n",
    "def get_mock_up_data():\n",
    "\n",
    "    ### Let's tokenize the vocabulary \n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(X_train)\n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'There are {vocab_size} different words in your corpus')\n",
    "    X_token = tk.texts_to_sequences(X_train)\n",
    "\n",
    "    ### Pad the inputs\n",
    "    X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "    \n",
    "    return X_pad, vocab_size\n",
    "\n",
    "X_pad, vocab_size = get_mock_up_data()\n",
    "print(\"X_pad.shape\", X_pad.shape)\n",
    "X_pad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL model using RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2627600   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                9680      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,637,301\n",
      "Trainable params: 2,637,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Let's build the neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, #+1 for the 0 padding\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer :)\n",
    "))\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "175/175 [==============================] - 128s 700ms/step - loss: 0.6796 - accuracy: 0.5720 - val_loss: 0.6683 - val_accuracy: 0.6100\n",
      "Epoch 2/100\n",
      "175/175 [==============================] - 109s 624ms/step - loss: 0.6154 - accuracy: 0.6721 - val_loss: 0.6520 - val_accuracy: 0.6121\n",
      "Epoch 3/100\n",
      "175/175 [==============================] - 104s 595ms/step - loss: 0.5574 - accuracy: 0.7300 - val_loss: 0.6962 - val_accuracy: 0.6164\n",
      "Epoch 4/100\n",
      "175/175 [==============================] - 104s 595ms/step - loss: 0.5022 - accuracy: 0.7713 - val_loss: 0.6980 - val_accuracy: 0.6050\n",
      "Epoch 5/100\n",
      "175/175 [==============================] - 103s 586ms/step - loss: 0.4577 - accuracy: 0.8020 - val_loss: 0.7524 - val_accuracy: 0.5929\n",
      "Epoch 6/100\n",
      "175/175 [==============================] - 101s 580ms/step - loss: 0.4093 - accuracy: 0.8288 - val_loss: 0.7999 - val_accuracy: 0.5900\n",
      "Epoch 7/100\n",
      "175/175 [==============================] - 96s 549ms/step - loss: 0.3729 - accuracy: 0.8477 - val_loss: 0.8708 - val_accuracy: 0.5843\n",
      "Epoch 8/100\n",
      "175/175 [==============================] - 86s 491ms/step - loss: 0.3327 - accuracy: 0.8680 - val_loss: 0.9048 - val_accuracy: 0.5871\n",
      "Epoch 9/100\n",
      "175/175 [==============================] - 98s 562ms/step - loss: 0.3039 - accuracy: 0.8795 - val_loss: 0.9779 - val_accuracy: 0.5807\n",
      "Epoch 10/100\n",
      "175/175 [==============================] - 85s 488ms/step - loss: 0.2760 - accuracy: 0.8936 - val_loss: 1.0039 - val_accuracy: 0.5814\n",
      "Epoch 11/100\n",
      "175/175 [==============================] - 96s 551ms/step - loss: 0.2451 - accuracy: 0.9057 - val_loss: 1.0240 - val_accuracy: 0.5871\n",
      "Epoch 12/100\n",
      "175/175 [==============================] - 89s 508ms/step - loss: 0.2184 - accuracy: 0.9186 - val_loss: 1.0443 - val_accuracy: 0.5771\n",
      "Epoch 13/100\n",
      "175/175 [==============================] - 88s 505ms/step - loss: 0.2028 - accuracy: 0.9295 - val_loss: 1.1235 - val_accuracy: 0.5893\n",
      "Epoch 14/100\n",
      "175/175 [==============================] - 92s 529ms/step - loss: 0.1883 - accuracy: 0.9312 - val_loss: 1.1652 - val_accuracy: 0.5771\n",
      "Epoch 15/100\n",
      "175/175 [==============================] - 88s 502ms/step - loss: 0.1785 - accuracy: 0.9350 - val_loss: 1.2485 - val_accuracy: 0.5771\n",
      "Epoch 16/100\n",
      "175/175 [==============================] - 84s 477ms/step - loss: 0.1551 - accuracy: 0.9463 - val_loss: 1.2294 - val_accuracy: 0.5686\n",
      "Epoch 17/100\n",
      "175/175 [==============================] - 92s 527ms/step - loss: 0.1513 - accuracy: 0.9484 - val_loss: 1.3189 - val_accuracy: 0.5693\n",
      "Epoch 18/100\n",
      "175/175 [==============================] - 88s 505ms/step - loss: 0.1334 - accuracy: 0.9543 - val_loss: 1.3486 - val_accuracy: 0.5593\n",
      "Epoch 19/100\n",
      "175/175 [==============================] - 114s 653ms/step - loss: 0.1292 - accuracy: 0.9579 - val_loss: 1.3560 - val_accuracy: 0.5764\n",
      "Epoch 20/100\n",
      "175/175 [==============================] - 105s 600ms/step - loss: 0.1215 - accuracy: 0.9596 - val_loss: 1.4112 - val_accuracy: 0.5750\n",
      "Epoch 21/100\n",
      "175/175 [==============================] - 106s 607ms/step - loss: 0.1137 - accuracy: 0.9591 - val_loss: 1.3646 - val_accuracy: 0.5764\n",
      "Epoch 22/100\n",
      "175/175 [==============================] - 113s 646ms/step - loss: 0.1098 - accuracy: 0.9616 - val_loss: 1.4627 - val_accuracy: 0.5707\n",
      "Epoch 23/100\n",
      "175/175 [==============================] - 105s 598ms/step - loss: 0.1072 - accuracy: 0.9625 - val_loss: 1.4950 - val_accuracy: 0.5743\n",
      "Epoch 24/100\n",
      "175/175 [==============================] - 107s 608ms/step - loss: 0.0985 - accuracy: 0.9688 - val_loss: 1.4607 - val_accuracy: 0.5800\n",
      "Epoch 25/100\n",
      "175/175 [==============================] - 103s 587ms/step - loss: 0.0884 - accuracy: 0.9707 - val_loss: 1.5940 - val_accuracy: 0.5800\n",
      "Epoch 26/100\n",
      "175/175 [==============================] - 101s 580ms/step - loss: 0.0877 - accuracy: 0.9698 - val_loss: 1.6211 - val_accuracy: 0.5736\n",
      "Epoch 27/100\n",
      "175/175 [==============================] - 104s 597ms/step - loss: 0.0782 - accuracy: 0.9718 - val_loss: 1.6749 - val_accuracy: 0.5743\n",
      "Epoch 28/100\n",
      "175/175 [==============================] - 98s 558ms/step - loss: 0.0777 - accuracy: 0.9755 - val_loss: 1.6484 - val_accuracy: 0.5636\n",
      "Epoch 29/100\n",
      "175/175 [==============================] - 107s 614ms/step - loss: 0.0749 - accuracy: 0.9755 - val_loss: 1.6115 - val_accuracy: 0.5643\n",
      "Epoch 30/100\n",
      "175/175 [==============================] - 102s 580ms/step - loss: 0.0681 - accuracy: 0.9791 - val_loss: 1.7305 - val_accuracy: 0.5636\n",
      "Epoch 31/100\n",
      "175/175 [==============================] - 108s 616ms/step - loss: 0.0708 - accuracy: 0.9759 - val_loss: 1.7725 - val_accuracy: 0.5843\n",
      "Epoch 32/100\n",
      "175/175 [==============================] - 119s 682ms/step - loss: 0.0684 - accuracy: 0.9773 - val_loss: 1.6451 - val_accuracy: 0.5886\n",
      "Epoch 33/100\n",
      "175/175 [==============================] - 112s 641ms/step - loss: 0.0662 - accuracy: 0.9795 - val_loss: 1.7595 - val_accuracy: 0.5686\n",
      "Epoch 34/100\n",
      "175/175 [==============================] - 107s 613ms/step - loss: 0.0605 - accuracy: 0.9805 - val_loss: 1.7788 - val_accuracy: 0.5614\n",
      "Epoch 35/100\n",
      "175/175 [==============================] - 104s 593ms/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 1.6202 - val_accuracy: 0.5786\n",
      "Epoch 36/100\n",
      "175/175 [==============================] - 97s 554ms/step - loss: 0.0598 - accuracy: 0.9809 - val_loss: 1.7976 - val_accuracy: 0.5707\n",
      "Epoch 37/100\n",
      "175/175 [==============================] - 106s 606ms/step - loss: 0.0543 - accuracy: 0.9818 - val_loss: 1.8256 - val_accuracy: 0.5614\n",
      "Epoch 38/100\n",
      "175/175 [==============================] - 123s 704ms/step - loss: 0.0610 - accuracy: 0.9818 - val_loss: 1.7292 - val_accuracy: 0.5686\n",
      "Epoch 39/100\n",
      "175/175 [==============================] - 94s 539ms/step - loss: 0.0553 - accuracy: 0.9812 - val_loss: 1.7791 - val_accuracy: 0.5536\n",
      "Epoch 40/100\n",
      "175/175 [==============================] - 87s 496ms/step - loss: 0.0494 - accuracy: 0.9820 - val_loss: 1.8386 - val_accuracy: 0.5764\n",
      "Epoch 41/100\n",
      "175/175 [==============================] - 97s 556ms/step - loss: 0.0537 - accuracy: 0.9820 - val_loss: 1.9465 - val_accuracy: 0.5814\n",
      "Epoch 42/100\n",
      "175/175 [==============================] - 86s 494ms/step - loss: 0.0507 - accuracy: 0.9827 - val_loss: 1.8914 - val_accuracy: 0.5771\n",
      "Epoch 43/100\n",
      "175/175 [==============================] - 89s 510ms/step - loss: 0.0567 - accuracy: 0.9812 - val_loss: 1.8462 - val_accuracy: 0.5721\n",
      "Epoch 44/100\n",
      "175/175 [==============================] - 127s 722ms/step - loss: 0.0509 - accuracy: 0.9823 - val_loss: 1.8563 - val_accuracy: 0.5714\n",
      "Epoch 45/100\n",
      "175/175 [==============================] - 110s 631ms/step - loss: 0.0473 - accuracy: 0.9825 - val_loss: 1.8535 - val_accuracy: 0.5729\n",
      "Epoch 46/100\n",
      "175/175 [==============================] - 97s 553ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 1.9910 - val_accuracy: 0.5686\n",
      "Epoch 47/100\n",
      "175/175 [==============================] - 111s 637ms/step - loss: 0.0464 - accuracy: 0.9836 - val_loss: 1.9081 - val_accuracy: 0.5714\n",
      "Epoch 48/100\n",
      "175/175 [==============================] - 91s 520ms/step - loss: 0.0435 - accuracy: 0.9852 - val_loss: 2.0391 - val_accuracy: 0.5721\n",
      "Epoch 49/100\n",
      "175/175 [==============================] - 94s 536ms/step - loss: 0.0452 - accuracy: 0.9843 - val_loss: 1.8716 - val_accuracy: 0.5571\n",
      "Epoch 50/100\n",
      "175/175 [==============================] - 89s 510ms/step - loss: 0.0415 - accuracy: 0.9845 - val_loss: 1.8134 - val_accuracy: 0.5593\n",
      "Epoch 51/100\n",
      "175/175 [==============================] - 87s 497ms/step - loss: 0.0439 - accuracy: 0.9850 - val_loss: 1.9625 - val_accuracy: 0.5543\n",
      "Epoch 52/100\n",
      "175/175 [==============================] - 102s 581ms/step - loss: 0.0414 - accuracy: 0.9839 - val_loss: 1.9201 - val_accuracy: 0.5757\n",
      "Epoch 53/100\n",
      "175/175 [==============================] - 136s 776ms/step - loss: 0.0404 - accuracy: 0.9855 - val_loss: 1.8928 - val_accuracy: 0.5750\n",
      "Epoch 54/100\n",
      "175/175 [==============================] - 98s 560ms/step - loss: 0.0386 - accuracy: 0.9859 - val_loss: 1.9488 - val_accuracy: 0.5464\n",
      "Epoch 55/100\n",
      "175/175 [==============================] - 88s 506ms/step - loss: 0.0413 - accuracy: 0.9850 - val_loss: 2.0193 - val_accuracy: 0.5571\n",
      "Epoch 56/100\n",
      "151/175 [========================>.....] - ETA: 14s - loss: 0.0362 - accuracy: 0.9870"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_pad, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL using CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_size = 100\n",
    "model_cnn = Sequential([\n",
    "    layers.Embedding(input_dim=7000, input_dim=vocab_size+1, output_dim=embedding_size, mask_zero=True),\n",
    "    layers.Conv1D(20, kernel_size=3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_pad, y_train, epochs=100, validation_split=0.2, batch_size=64, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML using SVM classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline_SVM = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    SVC()\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline_SVM, X_train, y_train, cv = 5, scoring = [\"accuracy\"])\n",
    "SVM_average_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "SVM_accuracy = np.round(SVM_average_accuracy,2)\n",
    "SVM_accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'SVC__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'SVC__C': (0.2, 0.5, 0.7)\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    parameters,\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
