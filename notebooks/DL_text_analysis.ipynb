{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>artist</th>\n",
       "      <th>seq</th>\n",
       "      <th>song</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>No, no\\r\\nI ain't ever trapped out the bando\\r...</td>\n",
       "      <td>Everyday</td>\n",
       "      <td>0.626</td>\n",
       "      <td>aint ever trap bando oh lord dont get wrong kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>The drinks go down and smoke goes up, I feel m...</td>\n",
       "      <td>Live Till We Die</td>\n",
       "      <td>0.630</td>\n",
       "      <td>drink go smoke go feel get let go care get los...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>She don't live on planet Earth no more\\r\\nShe ...</td>\n",
       "      <td>The Otherside</td>\n",
       "      <td>0.240</td>\n",
       "      <td>dont live planet earth find love venus thats w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>Trippin' off that Grigio, mobbin', lights low\\...</td>\n",
       "      <td>Pinot</td>\n",
       "      <td>0.536</td>\n",
       "      <td>trippin grigio mobbin light low trippin grigio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Elijah Blake</td>\n",
       "      <td>I see a midnight panther, so gallant and so br...</td>\n",
       "      <td>Shadows &amp; Diamonds</td>\n",
       "      <td>0.371</td>\n",
       "      <td>see midnight panther gallant brave find find a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0        artist  \\\n",
       "0             0           0  Elijah Blake   \n",
       "1             1           1  Elijah Blake   \n",
       "2             2           2  Elijah Blake   \n",
       "3             3           3  Elijah Blake   \n",
       "4             4           4  Elijah Blake   \n",
       "\n",
       "                                                 seq                song  \\\n",
       "0  No, no\\r\\nI ain't ever trapped out the bando\\r...            Everyday   \n",
       "1  The drinks go down and smoke goes up, I feel m...    Live Till We Die   \n",
       "2  She don't live on planet Earth no more\\r\\nShe ...       The Otherside   \n",
       "3  Trippin' off that Grigio, mobbin', lights low\\...               Pinot   \n",
       "4  I see a midnight panther, so gallant and so br...  Shadows & Diamonds   \n",
       "\n",
       "   label                                     cleaned_lyrics  \n",
       "0  0.626  aint ever trap bando oh lord dont get wrong kn...  \n",
       "1  0.630  drink go smoke go feel get let go care get los...  \n",
       "2  0.240  dont live planet earth find love venus thats w...  \n",
       "3  0.536  trippin grigio mobbin light low trippin grigio...  \n",
       "4  0.371  see midnight panther gallant brave find find a...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/labeled_lyrics_cleaned_processed.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_lyrics'].isna().sum()\n",
    "data['cleaned_lyrics'] = data['cleaned_lyrics'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = data.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5220\n",
       "1    4780\n",
       "Name: mood, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cat_valence(row):\n",
    "    if row >= 0.5:\n",
    "        return 1\n",
    "    elif row <0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "data_sub['mood'] = data_sub['label'].apply(lambda x:cat_valence(x))\n",
    "data_sub['mood'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Feature/Target\n",
    "X = data_sub['cleaned_lyrics'].apply(lambda x: np.str_(x))\n",
    "y = data_sub[\"mood\"]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline using logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    LogisticRegression()\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline, X_train, y_train, cv = 5, scoring = [\"accuracy\"])\n",
    "average_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "baseline_accuracy = np.round(average_accuracy,2)\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "10 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/pipeline.py\", line 401, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/pipeline.py\", line 359, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/joblib/memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 2133, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/feature_extraction/text.py\", line 1398, in fit_transform\n",
      "    raise ValueError(\"max_df corresponds to < documents than min_df\")\n",
      "ValueError: max_df corresponds to < documents than min_df\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/anais/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.52771429 0.54228571 0.58042857 0.60457143\n",
      " 0.58042857 0.596     ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score = 0.6045714285714285\n",
      "Best params = {'tfidfvectorizer__max_df': 0.3, 'tfidfvectorizer__max_features': 50, 'tfidfvectorizer__ngram_range': (1, 2)}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'tfidfvectorizer__ngram_range': ((2,2), (1,2)),\n",
    "    'tfidfvectorizer__max_df': [0,25, 0.3, 0.35],\n",
    "    'tfidfvectorizer__max_features': [50],\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    parameters,\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After different iterations, best params: max_df= 0.35, max_features=50, n_gram=(1,2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.35,max_features=50)\n",
    "\n",
    "vectorized_documents = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-14 17:10:19.833902: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-14 17:10:19.880044: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-14 17:10:19.880693: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-14 17:10:21.076263: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26752 different words in your corpus\n",
      "X_pad.shape (7000, 943)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.640e+02, 4.900e+01, 2.590e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [1.000e+00, 1.067e+03, 1.842e+03, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.220e+02, 2.892e+03, 5.000e+00, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       ...,\n",
       "       [9.200e+01, 1.326e+03, 9.660e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.600e+01, 4.377e+03, 2.870e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00],\n",
       "       [4.090e+02, 8.000e+00, 5.740e+02, ..., 0.000e+00, 0.000e+00,\n",
       "        0.000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "### Let's create some mock data\n",
    "def get_mock_up_data():\n",
    "\n",
    "    ### Let's tokenize the vocabulary \n",
    "    tk = Tokenizer()\n",
    "    tk.fit_on_texts(X_train)\n",
    "    vocab_size = len(tk.word_index)\n",
    "    print(f'There are {vocab_size} different words in your corpus')\n",
    "    X_token = tk.texts_to_sequences(X_train)\n",
    "\n",
    "    ### Pad the inputs\n",
    "    X_pad = pad_sequences(X_token, dtype='float32', padding='post')\n",
    "    \n",
    "    return X_pad, vocab_size\n",
    "\n",
    "X_pad, vocab_size = get_mock_up_data()\n",
    "print(\"X_pad.shape\", X_pad.shape)\n",
    "X_pad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL model using RNN (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         2675300   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 20)                9680      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,685,001\n",
      "Trainable params: 2,685,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### Let's build the neural network now\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(\n",
    "    input_dim=vocab_size+1, #+1 for the 0 padding\n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=True, # Built-in masking layer :)\n",
    "))\n",
    "\n",
    "model.add(layers.LSTM(20))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "88/88 [==============================] - 92s 994ms/step - loss: 0.6811 - accuracy: 0.5632 - val_loss: 0.6791 - val_accuracy: 0.6043\n",
      "Epoch 2/100\n",
      "88/88 [==============================] - 89s 1s/step - loss: 0.6286 - accuracy: 0.6584 - val_loss: 0.6547 - val_accuracy: 0.6257\n",
      "Epoch 3/100\n",
      "88/88 [==============================] - 79s 897ms/step - loss: 0.5757 - accuracy: 0.7211 - val_loss: 0.6704 - val_accuracy: 0.6257\n",
      "Epoch 4/100\n",
      "88/88 [==============================] - 74s 843ms/step - loss: 0.5336 - accuracy: 0.7568 - val_loss: 0.6902 - val_accuracy: 0.6171\n",
      "Epoch 5/100\n",
      "88/88 [==============================] - 77s 872ms/step - loss: 0.5072 - accuracy: 0.7766 - val_loss: 0.7149 - val_accuracy: 0.6164\n",
      "Epoch 6/100\n",
      "88/88 [==============================] - 79s 904ms/step - loss: 0.4730 - accuracy: 0.8012 - val_loss: 0.7281 - val_accuracy: 0.6157\n",
      "Epoch 7/100\n",
      "88/88 [==============================] - 86s 975ms/step - loss: 0.4357 - accuracy: 0.8221 - val_loss: 0.7353 - val_accuracy: 0.6286\n",
      "Epoch 8/100\n",
      "88/88 [==============================] - 88s 998ms/step - loss: 0.4020 - accuracy: 0.8402 - val_loss: 0.7958 - val_accuracy: 0.6086\n",
      "Epoch 9/100\n",
      "88/88 [==============================] - 82s 929ms/step - loss: 0.3638 - accuracy: 0.8604 - val_loss: 0.8288 - val_accuracy: 0.6021\n",
      "Epoch 10/100\n",
      "88/88 [==============================] - 87s 992ms/step - loss: 0.3288 - accuracy: 0.8741 - val_loss: 0.9108 - val_accuracy: 0.6014\n",
      "Epoch 11/100\n",
      "88/88 [==============================] - 82s 930ms/step - loss: 0.3059 - accuracy: 0.8825 - val_loss: 0.8943 - val_accuracy: 0.6057\n",
      "Epoch 12/100\n",
      "88/88 [==============================] - 86s 977ms/step - loss: 0.2787 - accuracy: 0.8986 - val_loss: 0.9739 - val_accuracy: 0.6007\n",
      "Epoch 13/100\n",
      "88/88 [==============================] - 92s 1s/step - loss: 0.2546 - accuracy: 0.9089 - val_loss: 1.0228 - val_accuracy: 0.5879\n",
      "Epoch 14/100\n",
      "88/88 [==============================] - 82s 936ms/step - loss: 0.2296 - accuracy: 0.9200 - val_loss: 1.0447 - val_accuracy: 0.5886\n",
      "Epoch 15/100\n",
      "88/88 [==============================] - 82s 934ms/step - loss: 0.2135 - accuracy: 0.9237 - val_loss: 1.0900 - val_accuracy: 0.5914\n",
      "Epoch 16/100\n",
      "88/88 [==============================] - 88s 995ms/step - loss: 0.1951 - accuracy: 0.9302 - val_loss: 1.1480 - val_accuracy: 0.5843\n",
      "Epoch 17/100\n",
      "88/88 [==============================] - 86s 974ms/step - loss: 0.1748 - accuracy: 0.9389 - val_loss: 1.1722 - val_accuracy: 0.5936\n",
      "Epoch 18/100\n",
      "88/88 [==============================] - 87s 990ms/step - loss: 0.1607 - accuracy: 0.9455 - val_loss: 1.2428 - val_accuracy: 0.5879\n",
      "Epoch 19/100\n",
      "88/88 [==============================] - 92s 1s/step - loss: 0.1492 - accuracy: 0.9507 - val_loss: 1.2420 - val_accuracy: 0.5829\n",
      "Epoch 20/100\n",
      "88/88 [==============================] - 108s 1s/step - loss: 0.1424 - accuracy: 0.9489 - val_loss: 1.2908 - val_accuracy: 0.5929\n",
      "Epoch 21/100\n",
      "88/88 [==============================] - 99s 1s/step - loss: 0.1269 - accuracy: 0.9571 - val_loss: 1.4278 - val_accuracy: 0.5864\n",
      "Epoch 22/100\n",
      "88/88 [==============================] - 82s 929ms/step - loss: 0.1251 - accuracy: 0.9563 - val_loss: 1.2946 - val_accuracy: 0.5779\n",
      "Epoch 23/100\n",
      "88/88 [==============================] - 77s 871ms/step - loss: 0.1190 - accuracy: 0.9616 - val_loss: 1.5189 - val_accuracy: 0.5850\n",
      "Epoch 24/100\n",
      "88/88 [==============================] - 83s 943ms/step - loss: 0.1026 - accuracy: 0.9648 - val_loss: 1.5146 - val_accuracy: 0.5786\n",
      "Epoch 25/100\n",
      "88/88 [==============================] - 76s 863ms/step - loss: 0.0970 - accuracy: 0.9668 - val_loss: 1.5656 - val_accuracy: 0.5907\n",
      "Epoch 26/100\n",
      "88/88 [==============================] - 77s 882ms/step - loss: 0.0916 - accuracy: 0.9671 - val_loss: 1.5282 - val_accuracy: 0.5764\n",
      "Epoch 27/100\n",
      "88/88 [==============================] - 87s 988ms/step - loss: 0.0888 - accuracy: 0.9693 - val_loss: 1.5493 - val_accuracy: 0.5814\n",
      "Epoch 28/100\n",
      "88/88 [==============================] - 89s 1s/step - loss: 0.0856 - accuracy: 0.9671 - val_loss: 1.5416 - val_accuracy: 0.5821\n",
      "Epoch 29/100\n",
      "88/88 [==============================] - 78s 891ms/step - loss: 0.0807 - accuracy: 0.9727 - val_loss: 1.5197 - val_accuracy: 0.5814\n",
      "Epoch 30/100\n",
      "88/88 [==============================] - 95s 1s/step - loss: 0.0767 - accuracy: 0.9720 - val_loss: 1.5773 - val_accuracy: 0.5636\n",
      "Epoch 31/100\n",
      "88/88 [==============================] - 94s 1s/step - loss: 0.0676 - accuracy: 0.9745 - val_loss: 1.6491 - val_accuracy: 0.5593\n",
      "Epoch 32/100\n",
      "88/88 [==============================] - 95s 1s/step - loss: 0.0681 - accuracy: 0.9761 - val_loss: 1.5939 - val_accuracy: 0.5829\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_pad, y_train, epochs=100, validation_split=0.2, batch_size=64, verbose=1, callbacks=[es])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "model_cnn = Sequential([\n",
    "    layers.Embedding(input_dim=7000, input_dim=vocab_size+1, output_dim=embedding_size, mask_zero=True),\n",
    "    layers.Conv1D(20, kernel_size=3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "\n",
    "es = EarlyStopping(monitor='loss', patience=1)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(X_pad, y_train, epochs=100, validation_split=0.2, batch_size=64, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML using SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel='linear', C=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline vectorizer + Naive Bayes\n",
    "pipeline_SVM = make_pipeline(\n",
    "    TfidfVectorizer(), \n",
    "    SVC()\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "cv_results = cross_validate(pipeline_SVM, X_train, y_train, cv = 5, scoring = [\"accuracy\"])\n",
    "SVM_average_accuracy = cv_results[\"test_accuracy\"].mean()\n",
    "SVM_accuracy = np.round(SVM_average_accuracy,2)\n",
    "SVM_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the grid of parameters\n",
    "parameters = {\n",
    "    'SVC__kernel': ('linear', 'poly', 'rbf', 'sigmoid'),\n",
    "    'SVC__C': (0.2, 0.5, 0.7)\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline,\n",
    "    parameters,\n",
    "    scoring = \"accuracy\",\n",
    "    cv = 5,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best score\n",
    "print(f\"Best Score = {grid_search.best_score_}\")\n",
    "\n",
    "# Best params\n",
    "print(f\"Best params = {grid_search.best_params_}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
