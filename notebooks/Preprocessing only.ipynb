{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps for Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114000, 24)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s1.csv')\n",
    "# df2 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s2.csv')\n",
    "# df3 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s3.csv')\n",
    "# df4 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s4.csv')\n",
    "# df5 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s5.csv')\n",
    "# df6 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s6.csv')\n",
    "# df7 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s7.csv')\n",
    "# df8 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s8.csv')\n",
    "# df9 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s9.csv')\n",
    "# df10 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s10.csv')\n",
    "# df11 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s11.csv')\n",
    "# frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n",
    "# data = pd.concat(frames)\n",
    "data = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_total.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39710, 24)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['lyrics_extracted']!='999']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics_language']=='en'] # we will have to decide whether to translate non english or use only english songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22833, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing steps and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING\n",
    "import unicodedata\n",
    "import re \n",
    "\n",
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    # function to remove accented characters\n",
    "    def remove_accented_chars(txt):\n",
    "        new_text = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return new_text\n",
    "    sentence = remove_accented_chars(sentence)\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "    \n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>...</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>lyrics_extracted</th>\n",
       "      <th>lyrics_language</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Youngblood thinks there's always tomorrow I mi...</td>\n",
       "      <td>en</td>\n",
       "      <td>youngblood think theres always tomorrow miss t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>When the world was ending, I'd hold you in my ...</td>\n",
       "      <td>en</td>\n",
       "      <td>world end id hold arm talk place wed never wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Wise men say ♪ Only fools rush in ♪ But I can'...</td>\n",
       "      <td>en</td>\n",
       "      <td>wise men say fool rush cant help fall love sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Loving and fighting, accusing, denying I can't...</td>\n",
       "      <td>en</td>\n",
       "      <td>love fight accuse deny cant imagine world go j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>01MVOl9KtVTNfFiBU9I7dc</td>\n",
       "      <td>Tyrone Wells</td>\n",
       "      <td>Days I Will Remember</td>\n",
       "      <td>Days I Will Remember</td>\n",
       "      <td>58</td>\n",
       "      <td>214240</td>\n",
       "      <td>False</td>\n",
       "      <td>0.688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.666</td>\n",
       "      <td>98.017</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>These are the days I will remember These are t...</td>\n",
       "      <td>en</td>\n",
       "      <td>days remember face need everythin change ill k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                track_id                 artists  \\\n",
       "1             1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2             2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3             3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4             4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "5             5           5  01MVOl9KtVTNfFiBU9I7dc            Tyrone Wells   \n",
       "\n",
       "                                          album_name  \\\n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "5                               Days I Will Remember   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "5        Days I Will Remember          58       214240     False   \n",
       "\n",
       "   danceability  ...  acousticness  instrumentalness  liveness  valence  \\\n",
       "1         0.420  ...         0.924          0.000006    0.1010    0.267   \n",
       "2         0.438  ...         0.210          0.000000    0.1170    0.120   \n",
       "3         0.266  ...         0.905          0.000071    0.1320    0.143   \n",
       "4         0.618  ...         0.469          0.000000    0.0829    0.167   \n",
       "5         0.688  ...         0.289          0.000000    0.1890    0.666   \n",
       "\n",
       "     tempo  time_signature  track_genre  \\\n",
       "1   77.489               4     acoustic   \n",
       "2   76.332               4     acoustic   \n",
       "3  181.740               3     acoustic   \n",
       "4  119.949               4     acoustic   \n",
       "5   98.017               4     acoustic   \n",
       "\n",
       "                                    lyrics_extracted  lyrics_language  \\\n",
       "1  Youngblood thinks there's always tomorrow I mi...               en   \n",
       "2  When the world was ending, I'd hold you in my ...               en   \n",
       "3  Wise men say ♪ Only fools rush in ♪ But I can'...               en   \n",
       "4  Loving and fighting, accusing, denying I can't...               en   \n",
       "5  These are the days I will remember These are t...               en   \n",
       "\n",
       "                                      cleaned_lyrics  \n",
       "1  youngblood think theres always tomorrow miss t...  \n",
       "2  world end id hold arm talk place wed never wor...  \n",
       "3  wise men say fool rush cant help fall love sha...  \n",
       "4  love fight accuse deny cant imagine world go j...  \n",
       "5  days remember face need everythin change ill k...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_lyrics'] = data[\"lyrics_extracted\"].apply(cleaning) # add signs (musical note) removal\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.35, max_features=50) # might be good to increase max_feat to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>away</th>\n",
       "      <th>baby</th>\n",
       "      <th>back</th>\n",
       "      <th>believe</th>\n",
       "      <th>cant</th>\n",
       "      <th>cause</th>\n",
       "      <th>could</th>\n",
       "      <th>day</th>\n",
       "      <th>every</th>\n",
       "      <th>...</th>\n",
       "      <th>us</th>\n",
       "      <th>wan</th>\n",
       "      <th>wan na</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wont</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503495</td>\n",
       "      <td>0.123739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.067760</td>\n",
       "      <td>0.218451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197820</td>\n",
       "      <td>0.200710</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164198</td>\n",
       "      <td>0.166597</td>\n",
       "      <td>0.199669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.337226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227697</td>\n",
       "      <td>0.458794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643899</td>\n",
       "      <td>0.107317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087248</td>\n",
       "      <td>0.081947</td>\n",
       "      <td>0.083144</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075348</td>\n",
       "      <td>0.084756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185572</td>\n",
       "      <td>0.374403</td>\n",
       "      <td>0.229403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575978</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       always      away      baby      back  believe      cant     cause  \\\n",
       "0    0.171728  0.000000  0.000000  0.000000      0.0  0.530493  0.000000   \n",
       "1    0.000000  0.112709  0.000000  0.201592      0.0  0.000000  0.096199   \n",
       "2    0.000000  0.000000  0.000000  0.000000      0.0  0.535017  0.000000   \n",
       "3    0.000000  0.061068  0.067760  0.218451      0.0  0.103337  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...      ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "997  0.337226  0.000000  0.227697  0.458794      0.0  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.000000  0.067870      0.0  0.064211  0.064775   \n",
       "999  0.000000  0.000000  0.000000  0.098073      0.0  0.185572  0.374403   \n",
       "\n",
       "        could       day     every  ...        us       wan    wan na  \\\n",
       "0    0.081974  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.353658  0.000000  ...  0.259149  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.197820  0.200710   \n",
       "4    0.000000  0.000000  0.000000  ...  0.000000  0.164198  0.166597   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.143016  ...  0.000000  0.000000  0.000000   \n",
       "997  0.643899  0.107317  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.079687  ...  0.087248  0.081947  0.083144   \n",
       "999  0.229403  0.000000  0.000000  ...  0.252149  0.000000  0.000000   \n",
       "\n",
       "         want       way      well      wont     world     would      yeah  \n",
       "0    0.137212  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.503495  0.123739  0.000000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.138845  0.000000  \n",
       "3    0.160369  0.000000  0.000000  0.000000  0.068200  0.000000  0.059057  \n",
       "4    0.199669  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.000000  0.221009  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "997  0.000000  0.187299  0.000000  0.114588  0.000000  0.000000  0.000000  \n",
       "998  0.066433  0.000000  0.075348  0.084756  0.000000  0.000000  0.073393  \n",
       "999  0.575978  0.100094  0.000000  0.000000  0.000000  0.000000  0.106053  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_vectors = pd.DataFrame(vectorizer.fit_transform(data['cleaned_lyrics']).toarray(),\n",
    "                       columns = vectorizer.get_feature_names_out())\n",
    "text_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two categories that correspond to positive mood (1) and 0(negative mood)\n",
    "# def cat_valence(row):\n",
    "#     if row >= 0.5:\n",
    "#         return 1\n",
    "#     elif row <0.5:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # applying the function to the valence column\n",
    "# y= data['mood'] = data['valence'].apply(lambda x:cat_valence(x))\n",
    "y = data['valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We are dropping :\n",
    "- descriptive variables: 'Unnamed: 0','track_id','artists','album_name','track_name'\n",
    "- valence/ mood which will be our target\n",
    "- acousticness and loudness that are highly correlated to energy (which we keep)\n",
    "- track_genre as it doesn't bring extra information\n",
    "'''\n",
    "feat_drop=['valence', 'Unnamed: 0.1', 'Unnamed: 0','track_id','artists','album_name','track_name','loudness','acousticness', 'track_genre', 'lyrics_extracted', 'lyrics_language', 'cleaned_lyrics']\n",
    "\n",
    "# Our features\n",
    "X = data.drop(columns=feat_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sub = X.iloc[:1000,:]\n",
    "# y_sub = y.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute then scale numerical values: \n",
    "num_transformer = Pipeline([('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# text_transformer = Pipeline(steps=[\n",
    "#     (\"squeez\", FunctionTransformer(lambda x: x.squeeze())),\n",
    "#     (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "#     (\"tfidf\", TfidfTransformer()),\n",
    "#     (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "# ])\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Parallelize \"num_transformer\" and \"cat_transfomer\"\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['popularity', 'duration_ms','danceability','energy','speechiness','instrumentalness','liveness','tempo']),\n",
    "    ('cat_transformer', cat_transformer, ['explicit', 'key','mode','time_signature'])\n",
    "    #,('text_transformer', text_transformer, ['cleaned_lyrics'])\n",
    "])\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "non_text_features =pd.DataFrame(X_transformed,columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([non_text_features, text_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 78)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 78), (200, 78), (800,), (200,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_sub, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
