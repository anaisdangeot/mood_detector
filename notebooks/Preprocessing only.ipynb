{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps for Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-16 12:01:55.797777: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-16 12:01:55.872187: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-16 12:01:55.874182: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-16 12:01:57.173986: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# GENERAL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.2', 'Unnamed: 0.1', 'Unnamed: 0', 'track_id', 'artists',\n",
       "       'album_name', 'track_name', 'popularity', 'duration_ms', 'explicit',\n",
       "       'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n",
       "       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo',\n",
       "       'time_signature', 'track_genre', 'lyrics_extracted', 'lyrics_language'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s1.csv')\n",
    "# df2 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s2.csv')\n",
    "# df3 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s3.csv')\n",
    "# df4 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s4.csv')\n",
    "# df5 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s5.csv')\n",
    "# df6 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s6.csv')\n",
    "# df7 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s7.csv')\n",
    "# df8 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s8.csv')\n",
    "# df9 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s9.csv')\n",
    "# df10 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s10.csv')\n",
    "# df11 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s11.csv')\n",
    "# frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n",
    "# data = pd.concat(frames)\n",
    "data = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_total.csv')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22833, 25)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['lyrics_extracted']!='999']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics_language']=='en'] # we will have to decide whether to translate non english or use only english songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22833, 25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing steps and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING\n",
    "import unicodedata\n",
    "import re \n",
    "\n",
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    # function to remove accented characters\n",
    "    def remove_accented_chars(txt):\n",
    "        new_text = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return new_text\n",
    "    sentence = remove_accented_chars(sentence)\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "    \n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of        Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0                track_id  \\\n",
       "0                 1             1           1  4qPNDBW1i3p13qLCt0Ki3A   \n",
       "1                 2             2           2  1iJBSr7s7jYXzM8EGcbK5b   \n",
       "2                 3             3           3  6lfxq3CG4xtTiEg7opyCyx   \n",
       "3                 4             4           4  5vjLSffimiIP26QG5WcN2K   \n",
       "4                 5             5           5  01MVOl9KtVTNfFiBU9I7dc   \n",
       "...             ...           ...         ...                     ...   \n",
       "22828          3987        113987      113987  4jDhzTYkEG5GloIWwVeVkc   \n",
       "22829          3988        113988      113988  6PM55W7WiUmHVPdUebJP55   \n",
       "22830          3991        113991      113991  0CE0Y6GM75cbrqao8EOAlW   \n",
       "22831          3992        113992      113992  3FjOBB4EyIXHYUtSgrIdY9   \n",
       "22832          3993        113993      113993  4OkMK49i3NApR1KsAIsTf6   \n",
       "\n",
       "                      artists  \\\n",
       "0                Ben Woodward   \n",
       "1      Ingrid Michaelson;ZAYN   \n",
       "2                Kina Grannis   \n",
       "3            Chord Overstreet   \n",
       "4                Tyrone Wells   \n",
       "...                       ...   \n",
       "22828           Planetshakers   \n",
       "22829           Planetshakers   \n",
       "22830            Chris Tomlin   \n",
       "22831           Jesus Culture   \n",
       "22832            Chris Tomlin   \n",
       "\n",
       "                                              album_name  \\\n",
       "0                                       Ghost (Acoustic)   \n",
       "1                                         To Begin Again   \n",
       "2      Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "3                                                Hold On   \n",
       "4                                   Days I Will Remember   \n",
       "...                                                  ...   \n",
       "22828                                     Revival (Live)   \n",
       "22829                                     Greater (Live)   \n",
       "22830                              The Ultimate Playlist   \n",
       "22831                                   Revelation Songs   \n",
       "22832                  See The Morning (Special Edition)   \n",
       "\n",
       "                        track_name  popularity  duration_ms  explicit  ...  \\\n",
       "0                 Ghost - Acoustic          55       149610     False  ...   \n",
       "1                   To Begin Again          57       210826     False  ...   \n",
       "2       Can't Help Falling In Love          71       201933     False  ...   \n",
       "3                          Hold On          82       198853     False  ...   \n",
       "4             Days I Will Remember          58       214240     False  ...   \n",
       "...                            ...         ...          ...       ...  ...   \n",
       "22828           All My Life - Live          40       246306     False  ...   \n",
       "22829   Stay (You Are Good) - Live          38       462397     False  ...   \n",
       "22830  At The Cross (Love Ran Red)          32       250629     False  ...   \n",
       "22831        Your Love Never Fails          38       312566     False  ...   \n",
       "22832  How Can I Keep From Singing          39       256026     False  ...   \n",
       "\n",
       "       acousticness  instrumentalness  liveness  valence    tempo  \\\n",
       "0          0.924000          0.000006    0.1010    0.267   77.489   \n",
       "1          0.210000          0.000000    0.1170    0.120   76.332   \n",
       "2          0.905000          0.000071    0.1320    0.143  181.740   \n",
       "3          0.469000          0.000000    0.0829    0.167  119.949   \n",
       "4          0.289000          0.000000    0.1890    0.666   98.017   \n",
       "...             ...               ...       ...      ...      ...   \n",
       "22828      0.000529          0.000000    0.2510    0.453  128.002   \n",
       "22829      0.072400          0.000003    0.3740    0.146  139.051   \n",
       "22830      0.003050          0.000000    0.2010    0.153  146.003   \n",
       "22831      0.006500          0.000002    0.2460    0.427  113.949   \n",
       "22832      0.084100          0.000000    0.1880    0.382  104.083   \n",
       "\n",
       "       time_signature  track_genre  \\\n",
       "0                   4     acoustic   \n",
       "1                   4     acoustic   \n",
       "2                   3     acoustic   \n",
       "3                   4     acoustic   \n",
       "4                   4     acoustic   \n",
       "...               ...          ...   \n",
       "22828               4  world-music   \n",
       "22829               4  world-music   \n",
       "22830               4  world-music   \n",
       "22831               4  world-music   \n",
       "22832               3  world-music   \n",
       "\n",
       "                                        lyrics_extracted  lyrics_language  \\\n",
       "0      Youngblood thinks there's always tomorrow I mi...               en   \n",
       "1      When the world was ending, I'd hold you in my ...               en   \n",
       "2      Wise men say ♪ Only fools rush in ♪ But I can'...               en   \n",
       "3      Loving and fighting, accusing, denying I can't...               en   \n",
       "4      These are the days I will remember These are t...               en   \n",
       "...                                                  ...              ...   \n",
       "22828  I'm not looking back to what once was My visio...               en   \n",
       "22829  I'll never leave You I've tasted Your goodness...               en   \n",
       "22830  There's a place where mercy reigns And never d...               en   \n",
       "22831  Nothing can separate Even if I ran away Your l...               en   \n",
       "22832  There is an endless song, echoes in my soul I ...               en   \n",
       "\n",
       "                                          cleaned_lyrics  \n",
       "0      youngblood think theres always tomorrow miss t...  \n",
       "1      world end id hold arm talk place wed never wor...  \n",
       "2      wise men say fool rush cant help fall love sha...  \n",
       "3      love fight accuse deny cant imagine world go j...  \n",
       "4      days remember face need everythin change ill k...  \n",
       "...                                                  ...  \n",
       "22828  im look back vision ahead know greater things ...  \n",
       "22829  ill never leave ive taste goodness nothing els...  \n",
       "22830  theres place mercy reign never die theres plac...  \n",
       "22831  nothing separate even run away love never fail...  \n",
       "22832  endless song echo soul hear music ring though ...  \n",
       "\n",
       "[22833 rows x 26 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_lyrics'] = data[\"lyrics_extracted\"].apply(cleaning) # add signs (musical note) removal\n",
    "data.head()\n",
    "data.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22833 entries, 0 to 22832\n",
      "Data columns (total 26 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0.2      22833 non-null  int64  \n",
      " 1   Unnamed: 0.1      22833 non-null  int64  \n",
      " 2   Unnamed: 0        22833 non-null  int64  \n",
      " 3   track_id          22833 non-null  object \n",
      " 4   artists           22833 non-null  object \n",
      " 5   album_name        22833 non-null  object \n",
      " 6   track_name        22833 non-null  object \n",
      " 7   popularity        22833 non-null  int64  \n",
      " 8   duration_ms       22833 non-null  int64  \n",
      " 9   explicit          22833 non-null  bool   \n",
      " 10  danceability      22833 non-null  float64\n",
      " 11  energy            22833 non-null  float64\n",
      " 12  key               22833 non-null  int64  \n",
      " 13  loudness          22833 non-null  float64\n",
      " 14  mode              22833 non-null  int64  \n",
      " 15  speechiness       22833 non-null  float64\n",
      " 16  acousticness      22833 non-null  float64\n",
      " 17  instrumentalness  22833 non-null  float64\n",
      " 18  liveness          22833 non-null  float64\n",
      " 19  valence           22833 non-null  float64\n",
      " 20  tempo             22833 non-null  float64\n",
      " 21  time_signature    22833 non-null  int64  \n",
      " 22  track_genre       22833 non-null  object \n",
      " 23  lyrics_extracted  22833 non-null  object \n",
      " 24  lyrics_language   22833 non-null  object \n",
      " 25  cleaned_lyrics    22833 non-null  object \n",
      "dtypes: bool(1), float64(9), int64(8), object(8)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.35, max_features=50) # might be good to increase max_feat to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>away</th>\n",
       "      <th>baby</th>\n",
       "      <th>back</th>\n",
       "      <th>believe</th>\n",
       "      <th>cant</th>\n",
       "      <th>cause</th>\n",
       "      <th>could</th>\n",
       "      <th>day</th>\n",
       "      <th>every</th>\n",
       "      <th>...</th>\n",
       "      <th>us</th>\n",
       "      <th>wan</th>\n",
       "      <th>wan na</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wont</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503495</td>\n",
       "      <td>0.123739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.067760</td>\n",
       "      <td>0.218451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197820</td>\n",
       "      <td>0.200710</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164198</td>\n",
       "      <td>0.166597</td>\n",
       "      <td>0.199669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.337226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227697</td>\n",
       "      <td>0.458794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643899</td>\n",
       "      <td>0.107317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087248</td>\n",
       "      <td>0.081947</td>\n",
       "      <td>0.083144</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075348</td>\n",
       "      <td>0.084756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185572</td>\n",
       "      <td>0.374403</td>\n",
       "      <td>0.229403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575978</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       always      away      baby      back  believe      cant     cause  \\\n",
       "0    0.171728  0.000000  0.000000  0.000000      0.0  0.530493  0.000000   \n",
       "1    0.000000  0.112709  0.000000  0.201592      0.0  0.000000  0.096199   \n",
       "2    0.000000  0.000000  0.000000  0.000000      0.0  0.535017  0.000000   \n",
       "3    0.000000  0.061068  0.067760  0.218451      0.0  0.103337  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...      ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "997  0.337226  0.000000  0.227697  0.458794      0.0  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.000000  0.067870      0.0  0.064211  0.064775   \n",
       "999  0.000000  0.000000  0.000000  0.098073      0.0  0.185572  0.374403   \n",
       "\n",
       "        could       day     every  ...        us       wan    wan na  \\\n",
       "0    0.081974  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.353658  0.000000  ...  0.259149  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.197820  0.200710   \n",
       "4    0.000000  0.000000  0.000000  ...  0.000000  0.164198  0.166597   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.143016  ...  0.000000  0.000000  0.000000   \n",
       "997  0.643899  0.107317  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.079687  ...  0.087248  0.081947  0.083144   \n",
       "999  0.229403  0.000000  0.000000  ...  0.252149  0.000000  0.000000   \n",
       "\n",
       "         want       way      well      wont     world     would      yeah  \n",
       "0    0.137212  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.503495  0.123739  0.000000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.138845  0.000000  \n",
       "3    0.160369  0.000000  0.000000  0.000000  0.068200  0.000000  0.059057  \n",
       "4    0.199669  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.000000  0.221009  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "997  0.000000  0.187299  0.000000  0.114588  0.000000  0.000000  0.000000  \n",
       "998  0.066433  0.000000  0.075348  0.084756  0.000000  0.000000  0.073393  \n",
       "999  0.575978  0.100094  0.000000  0.000000  0.000000  0.000000  0.106053  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_vectors = pd.DataFrame(vectorizer.fit_transform(data['cleaned_lyrics']).toarray(),\n",
    "                       columns = vectorizer.get_feature_names_out())\n",
    "text_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two categories that correspond to positive mood (1) and 0(negative mood)\n",
    "# def cat_valence(row):\n",
    "#     if row >= 0.5:\n",
    "#         return 1\n",
    "#     elif row <0.5:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "# # applying the function to the valence column\n",
    "# y= data['mood'] = data['valence'].apply(lambda x:cat_valence(x))\n",
    "y = data['valence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We are dropping :\n",
    "- descriptive variables: 'Unnamed: 0','track_id','artists','album_name','track_name'\n",
    "- valence/ mood which will be our target\n",
    "- acousticness and loudness that are highly correlated to energy (which we keep)\n",
    "- track_genre as it doesn't bring extra information\n",
    "'''\n",
    "feat_drop=['valence', 'Unnamed: 0.1', 'Unnamed: 0','track_id','artists','album_name','track_name','loudness','acousticness', 'track_genre', 'lyrics_extracted', 'lyrics_language', 'cleaned_lyrics']\n",
    "\n",
    "# Our features\n",
    "X = data.drop(columns=feat_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sub = X.iloc[:1000,:]\n",
    "# y_sub = y.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute then scale numerical values: \n",
    "num_transformer = Pipeline([('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# text_transformer = Pipeline(steps=[\n",
    "#     (\"squeez\", FunctionTransformer(lambda x: x.squeeze())),\n",
    "#     (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "#     (\"tfidf\", TfidfTransformer()),\n",
    "#     (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "# ])\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Parallelize \"num_transformer\" and \"cat_transfomer\"\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['popularity', 'duration_ms','danceability','energy','speechiness','instrumentalness','liveness','tempo']),\n",
    "    ('cat_transformer', cat_transformer, ['explicit', 'key','mode','time_signature'])\n",
    "    #,('text_transformer', text_transformer, ['cleaned_lyrics'])\n",
    "])\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "non_text_features =pd.DataFrame(X_transformed,columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([non_text_features, text_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 78)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 78), (200, 78), (800,), (200,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_sub, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
