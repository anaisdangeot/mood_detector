{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Steps (to be packaged)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sklearn \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Language processing\n",
    "import nltk\n",
    "from langdetect import detect\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114000, 24)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df1 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s1.csv')\n",
    "# df2 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s2.csv')\n",
    "# df3 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s3.csv')\n",
    "# df4 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s4.csv')\n",
    "# df5 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s5.csv')\n",
    "# df6 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s6.csv')\n",
    "# df7 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s7.csv')\n",
    "# df8 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s8.csv')\n",
    "# df9 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s9.csv')\n",
    "# df10 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s10.csv')\n",
    "# df11 = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_s11.csv')\n",
    "# frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9, df10, df11]\n",
    "# data = pd.concat(frames)\n",
    "data = pd.read_csv('/home/anais/code/anaisdangeot/mood_detector/raw_data/dataset_enriched_total.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39710, 24)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[data['lyrics_extracted']!='999']\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['lyrics_language']=='en'] # we will have to decide whether to translate non english or use only english songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22833, 24)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing steps and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING\n",
    "import unicodedata\n",
    "import re \n",
    "\n",
    "def cleaning(sentence):\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    # function to remove accented characters\n",
    "    def remove_accented_chars(txt):\n",
    "        new_text = unicodedata.normalize('NFKD', txt).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return new_text\n",
    "    sentence = remove_accented_chars(sentence)\n",
    "    \n",
    "    tokenized_sentence = nltk.word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "    \n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>...</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>lyrics_extracted</th>\n",
       "      <th>lyrics_language</th>\n",
       "      <th>cleaned_lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>...</td>\n",
       "      <td>0.924</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Youngblood thinks there's always tomorrow I mi...</td>\n",
       "      <td>en</td>\n",
       "      <td>youngblood think theres always tomorrow miss t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>When the world was ending, I'd hold you in my ...</td>\n",
       "      <td>en</td>\n",
       "      <td>world end id hold arm talk place wed never wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Wise men say ♪ Only fools rush in ♪ But I can'...</td>\n",
       "      <td>en</td>\n",
       "      <td>wise men say fool rush cant help fall love sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>...</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>Loving and fighting, accusing, denying I can't...</td>\n",
       "      <td>en</td>\n",
       "      <td>love fight accuse deny cant imagine world go j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>01MVOl9KtVTNfFiBU9I7dc</td>\n",
       "      <td>Tyrone Wells</td>\n",
       "      <td>Days I Will Remember</td>\n",
       "      <td>Days I Will Remember</td>\n",
       "      <td>58</td>\n",
       "      <td>214240</td>\n",
       "      <td>False</td>\n",
       "      <td>0.688</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.666</td>\n",
       "      <td>98.017</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>These are the days I will remember These are t...</td>\n",
       "      <td>en</td>\n",
       "      <td>days remember face need everythin change ill k...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0                track_id                 artists  \\\n",
       "1             1           1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2             2           2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3             3           3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4             4           4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "5             5           5  01MVOl9KtVTNfFiBU9I7dc            Tyrone Wells   \n",
       "\n",
       "                                          album_name  \\\n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "5                               Days I Will Remember   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "1            Ghost - Acoustic          55       149610     False   \n",
       "2              To Begin Again          57       210826     False   \n",
       "3  Can't Help Falling In Love          71       201933     False   \n",
       "4                     Hold On          82       198853     False   \n",
       "5        Days I Will Remember          58       214240     False   \n",
       "\n",
       "   danceability  ...  acousticness  instrumentalness  liveness  valence  \\\n",
       "1         0.420  ...         0.924          0.000006    0.1010    0.267   \n",
       "2         0.438  ...         0.210          0.000000    0.1170    0.120   \n",
       "3         0.266  ...         0.905          0.000071    0.1320    0.143   \n",
       "4         0.618  ...         0.469          0.000000    0.0829    0.167   \n",
       "5         0.688  ...         0.289          0.000000    0.1890    0.666   \n",
       "\n",
       "     tempo  time_signature  track_genre  \\\n",
       "1   77.489               4     acoustic   \n",
       "2   76.332               4     acoustic   \n",
       "3  181.740               3     acoustic   \n",
       "4  119.949               4     acoustic   \n",
       "5   98.017               4     acoustic   \n",
       "\n",
       "                                    lyrics_extracted  lyrics_language  \\\n",
       "1  Youngblood thinks there's always tomorrow I mi...               en   \n",
       "2  When the world was ending, I'd hold you in my ...               en   \n",
       "3  Wise men say ♪ Only fools rush in ♪ But I can'...               en   \n",
       "4  Loving and fighting, accusing, denying I can't...               en   \n",
       "5  These are the days I will remember These are t...               en   \n",
       "\n",
       "                                      cleaned_lyrics  \n",
       "1  youngblood think theres always tomorrow miss t...  \n",
       "2  world end id hold arm talk place wed never wor...  \n",
       "3  wise men say fool rush cant help fall love sha...  \n",
       "4  love fight accuse deny cant imagine world go j...  \n",
       "5  days remember face need everythin change ill k...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_lyrics'] = data[\"lyrics_extracted\"].apply(cleaning) # add signs (musical note) removal\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.35, max_features=50) # might be good to increase max_feat to improve score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>always</th>\n",
       "      <th>away</th>\n",
       "      <th>baby</th>\n",
       "      <th>back</th>\n",
       "      <th>believe</th>\n",
       "      <th>cant</th>\n",
       "      <th>cause</th>\n",
       "      <th>could</th>\n",
       "      <th>day</th>\n",
       "      <th>every</th>\n",
       "      <th>...</th>\n",
       "      <th>us</th>\n",
       "      <th>wan</th>\n",
       "      <th>wan na</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wont</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.171728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.530493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.081974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.137212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112709</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.503495</td>\n",
       "      <td>0.123739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.535017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138845</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061068</td>\n",
       "      <td>0.067760</td>\n",
       "      <td>0.218451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.103337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.197820</td>\n",
       "      <td>0.200710</td>\n",
       "      <td>0.160369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.164198</td>\n",
       "      <td>0.166597</td>\n",
       "      <td>0.199669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.221009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.143016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.337226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227697</td>\n",
       "      <td>0.458794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.643899</td>\n",
       "      <td>0.107317</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.067870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064211</td>\n",
       "      <td>0.064775</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087248</td>\n",
       "      <td>0.081947</td>\n",
       "      <td>0.083144</td>\n",
       "      <td>0.066433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075348</td>\n",
       "      <td>0.084756</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.185572</td>\n",
       "      <td>0.374403</td>\n",
       "      <td>0.229403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252149</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575978</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       always      away      baby      back  believe      cant     cause  \\\n",
       "0    0.171728  0.000000  0.000000  0.000000      0.0  0.530493  0.000000   \n",
       "1    0.000000  0.112709  0.000000  0.201592      0.0  0.000000  0.096199   \n",
       "2    0.000000  0.000000  0.000000  0.000000      0.0  0.535017  0.000000   \n",
       "3    0.000000  0.061068  0.067760  0.218451      0.0  0.103337  0.000000   \n",
       "4    0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "..        ...       ...       ...       ...      ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.000000  0.000000      0.0  0.000000  0.000000   \n",
       "997  0.337226  0.000000  0.227697  0.458794      0.0  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.000000  0.067870      0.0  0.064211  0.064775   \n",
       "999  0.000000  0.000000  0.000000  0.098073      0.0  0.185572  0.374403   \n",
       "\n",
       "        could       day     every  ...        us       wan    wan na  \\\n",
       "0    0.081974  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1    0.000000  0.353658  0.000000  ...  0.259149  0.000000  0.000000   \n",
       "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3    0.000000  0.000000  0.000000  ...  0.000000  0.197820  0.200710   \n",
       "4    0.000000  0.000000  0.000000  ...  0.000000  0.164198  0.166597   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "996  0.000000  0.000000  0.143016  ...  0.000000  0.000000  0.000000   \n",
       "997  0.643899  0.107317  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "998  0.000000  0.000000  0.079687  ...  0.087248  0.081947  0.083144   \n",
       "999  0.229403  0.000000  0.000000  ...  0.252149  0.000000  0.000000   \n",
       "\n",
       "         want       way      well      wont     world     would      yeah  \n",
       "0    0.137212  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.503495  0.123739  0.000000  \n",
       "2    0.000000  0.000000  0.000000  0.000000  0.000000  0.138845  0.000000  \n",
       "3    0.160369  0.000000  0.000000  0.000000  0.068200  0.000000  0.059057  \n",
       "4    0.199669  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.000000  0.221009  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "996  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "997  0.000000  0.187299  0.000000  0.114588  0.000000  0.000000  0.000000  \n",
       "998  0.066433  0.000000  0.075348  0.084756  0.000000  0.000000  0.073393  \n",
       "999  0.575978  0.100094  0.000000  0.000000  0.000000  0.000000  0.106053  \n",
       "\n",
       "[1000 rows x 50 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "text_vectors = pd.DataFrame(vectorizer.fit_transform(data['cleaned_lyrics']).toarray(),\n",
    "                       columns = vectorizer.get_feature_names_out())\n",
    "text_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Non text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create two categories that correspond to positive mood (1) and 0(negative mood)\n",
    "def cat_valence(row):\n",
    "    if row >= 0.5:\n",
    "        return 1\n",
    "    elif row <0.5:\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# applying the function to the valence column\n",
    "data['mood'] = data['valence'].apply(lambda x:cat_valence(x))\n",
    "y = data['mood']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We are dropping :\n",
    "- descriptive variables: 'Unnamed: 0','track_id','artists','album_name','track_name'\n",
    "- valence/ mood which will be our target\n",
    "- acousticness and loudness that are highly correlated to energy (which we keep)\n",
    "- track_genre as it doesn't bring extra information\n",
    "'''\n",
    "feat_drop=['valence', 'mood', 'Unnamed: 0.1', 'Unnamed: 0','track_id','artists','album_name','track_name','loudness','acousticness', 'track_genre', 'lyrics_extracted', 'lyrics_language', 'cleaned_lyrics']\n",
    "\n",
    "# Our features\n",
    "X = data.drop(columns=feat_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sub = X.iloc[:1000,:]\n",
    "# y_sub = y.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute then scale numerical values: \n",
    "num_transformer = Pipeline([('min_max_scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "vectorizer_params = dict(ngram_range=(1, 2), min_df=5, max_df=0.8)\n",
    "\n",
    "# text_transformer = Pipeline(steps=[\n",
    "#     (\"squeez\", FunctionTransformer(lambda x: x.squeeze())),\n",
    "#     (\"vect\", CountVectorizer(**vectorizer_params)),\n",
    "#     (\"tfidf\", TfidfTransformer()),\n",
    "#     (\"toarray\", FunctionTransformer(lambda x: x.toarray())),\n",
    "# ])\n",
    "# Encode categorical values\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Parallelize \"num_transformer\" and \"cat_transfomer\"\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_transformer', num_transformer, ['popularity', 'duration_ms','danceability','energy','speechiness','instrumentalness','liveness','tempo']),\n",
    "    ('cat_transformer', cat_transformer, ['explicit', 'key','mode','time_signature'])\n",
    "    #,('text_transformer', text_transformer, ['cleaned_lyrics'])\n",
    "])\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "non_text_features =pd.DataFrame(X_transformed,columns=preprocessor.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = pd.concat([non_text_features, text_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 78)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800, 78), (200, 78), (800,), (200,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_sub, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_logR = LogisticRegression(max_iter=1000)\n",
    "\n",
    "model_logR.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logR.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_SVC = SVC(kernel='rbf')\n",
    "\n",
    "model_SVC.fit(X_train, y_train)\n",
    "\n",
    "model_SVC.score(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 200)         15600     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 20)                17680     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,301\n",
      "Trainable params: 33,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Size of your embedding space = size of the vector representing each word\n",
    "embedding_size = 200\n",
    "\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(layers.Embedding(\n",
    "    input_dim=X_train.shape[1], \n",
    "    output_dim=embedding_size, # 100\n",
    "    mask_zero=False, # Built-in masking layer :)\n",
    "))\n",
    "\n",
    "model_rnn.add(layers.LSTM(20))\n",
    "model_rnn.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "20/20 [==============================] - 8s 161ms/step - loss: 0.6725 - accuracy: 0.5969 - val_loss: 0.6620 - val_accuracy: 0.6438\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 2s 90ms/step - loss: 0.6675 - accuracy: 0.6109 - val_loss: 0.6528 - val_accuracy: 0.6438\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 2s 84ms/step - loss: 0.6679 - accuracy: 0.6125 - val_loss: 0.6551 - val_accuracy: 0.6438\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 2s 83ms/step - loss: 0.6667 - accuracy: 0.6125 - val_loss: 0.6595 - val_accuracy: 0.6438\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 0.6669 - accuracy: 0.6125 - val_loss: 0.6588 - val_accuracy: 0.6438\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 2s 94ms/step - loss: 0.6669 - accuracy: 0.6141 - val_loss: 0.6573 - val_accuracy: 0.6500\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.6657 - accuracy: 0.6187 - val_loss: 0.6568 - val_accuracy: 0.6500\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.6648 - accuracy: 0.6172 - val_loss: 0.6595 - val_accuracy: 0.6500\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 2s 92ms/step - loss: 0.6645 - accuracy: 0.6187 - val_loss: 0.6554 - val_accuracy: 0.6500\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 2s 79ms/step - loss: 0.6653 - accuracy: 0.6203 - val_loss: 0.6565 - val_accuracy: 0.6500\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 1s 73ms/step - loss: 0.6642 - accuracy: 0.6172 - val_loss: 0.6585 - val_accuracy: 0.6500\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 2s 79ms/step - loss: 0.6649 - accuracy: 0.6172 - val_loss: 0.6558 - val_accuracy: 0.6500\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 2s 86ms/step - loss: 0.6634 - accuracy: 0.6172 - val_loss: 0.6607 - val_accuracy: 0.6375\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 2s 102ms/step - loss: 0.6653 - accuracy: 0.6187 - val_loss: 0.6579 - val_accuracy: 0.6500\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 2s 100ms/step - loss: 0.6636 - accuracy: 0.6172 - val_loss: 0.6582 - val_accuracy: 0.6500\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.6637 - accuracy: 0.6203 - val_loss: 0.6556 - val_accuracy: 0.6500\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.6625 - accuracy: 0.6156 - val_loss: 0.6586 - val_accuracy: 0.6375\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 0.6630 - accuracy: 0.6203 - val_loss: 0.6588 - val_accuracy: 0.6375\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 2s 97ms/step - loss: 0.6645 - accuracy: 0.6250 - val_loss: 0.6565 - val_accuracy: 0.6500\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 2s 97ms/step - loss: 0.6631 - accuracy: 0.6234 - val_loss: 0.6547 - val_accuracy: 0.6500\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 2s 88ms/step - loss: 0.6635 - accuracy: 0.6172 - val_loss: 0.6562 - val_accuracy: 0.6375\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 2s 87ms/step - loss: 0.6622 - accuracy: 0.6266 - val_loss: 0.6549 - val_accuracy: 0.6500\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 2s 106ms/step - loss: 0.6629 - accuracy: 0.6281 - val_loss: 0.6557 - val_accuracy: 0.6500\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 2s 99ms/step - loss: 0.6632 - accuracy: 0.6234 - val_loss: 0.6547 - val_accuracy: 0.6500\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 3s 129ms/step - loss: 0.6616 - accuracy: 0.6234 - val_loss: 0.6582 - val_accuracy: 0.6375\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 3s 127ms/step - loss: 0.6634 - accuracy: 0.6187 - val_loss: 0.6558 - val_accuracy: 0.6375\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.6622 - accuracy: 0.6234 - val_loss: 0.6551 - val_accuracy: 0.6500\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.6634 - accuracy: 0.6172 - val_loss: 0.6566 - val_accuracy: 0.6375\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 0.6625 - accuracy: 0.6250 - val_loss: 0.6544 - val_accuracy: 0.6500\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 3s 131ms/step - loss: 0.6626 - accuracy: 0.6250 - val_loss: 0.6544 - val_accuracy: 0.6500\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 2s 124ms/step - loss: 0.6625 - accuracy: 0.6250 - val_loss: 0.6535 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 2s 114ms/step - loss: 0.6623 - accuracy: 0.6187 - val_loss: 0.6550 - val_accuracy: 0.6375\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 2s 108ms/step - loss: 0.6627 - accuracy: 0.6266 - val_loss: 0.6542 - val_accuracy: 0.6500\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 2s 91ms/step - loss: 0.6627 - accuracy: 0.6172 - val_loss: 0.6542 - val_accuracy: 0.6375\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 2s 104ms/step - loss: 0.6618 - accuracy: 0.6219 - val_loss: 0.6570 - val_accuracy: 0.6375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e842e8ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=10)\n",
    "model_rnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "model_rnn.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, verbose=1, callbacks=[es])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***RNN model evaluation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 2s 24ms/step - loss: 0.6499 - accuracy: 0.6550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6499391198158264, 0.6549999713897705]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.evaluate(X_test,y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_transformer__popularity</th>\n",
       "      <th>num_transformer__duration_ms</th>\n",
       "      <th>num_transformer__danceability</th>\n",
       "      <th>num_transformer__energy</th>\n",
       "      <th>num_transformer__speechiness</th>\n",
       "      <th>num_transformer__instrumentalness</th>\n",
       "      <th>num_transformer__liveness</th>\n",
       "      <th>num_transformer__tempo</th>\n",
       "      <th>cat_transformer__explicit_False</th>\n",
       "      <th>cat_transformer__explicit_True</th>\n",
       "      <th>...</th>\n",
       "      <th>us</th>\n",
       "      <th>wan</th>\n",
       "      <th>wan na</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wont</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>yeah</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.795699</td>\n",
       "      <td>0.227306</td>\n",
       "      <td>0.660256</td>\n",
       "      <td>0.360435</td>\n",
       "      <td>0.015794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.071724</td>\n",
       "      <td>0.303660</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055069</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227711</td>\n",
       "      <td>0.610256</td>\n",
       "      <td>0.857304</td>\n",
       "      <td>0.124770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298655</td>\n",
       "      <td>0.436410</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073473</td>\n",
       "      <td>0.076609</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0.516129</td>\n",
       "      <td>0.139312</td>\n",
       "      <td>0.721795</td>\n",
       "      <td>0.273175</td>\n",
       "      <td>0.029481</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.094184</td>\n",
       "      <td>0.462260</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247231</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>0.279570</td>\n",
       "      <td>0.168962</td>\n",
       "      <td>0.775641</td>\n",
       "      <td>0.556514</td>\n",
       "      <td>0.055014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.089946</td>\n",
       "      <td>0.489139</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.199739</td>\n",
       "      <td>0.297436</td>\n",
       "      <td>0.943538</td>\n",
       "      <td>0.145301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069075</td>\n",
       "      <td>0.621421</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.243225</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.216160</td>\n",
       "      <td>0.698718</td>\n",
       "      <td>0.429217</td>\n",
       "      <td>0.021848</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.100540</td>\n",
       "      <td>0.251327</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.125102</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.229609</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300954</td>\n",
       "      <td>0.066275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.227552</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.563700</td>\n",
       "      <td>0.011319</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.163047</td>\n",
       "      <td>0.195223</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.264154</td>\n",
       "      <td>0.737179</td>\n",
       "      <td>0.494918</td>\n",
       "      <td>0.072124</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.065261</td>\n",
       "      <td>0.259910</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.237914</td>\n",
       "      <td>0.241388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>0.602151</td>\n",
       "      <td>0.086616</td>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.330664</td>\n",
       "      <td>0.042116</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.093124</td>\n",
       "      <td>0.686135</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435210</td>\n",
       "      <td>0.157803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>0.387097</td>\n",
       "      <td>0.190209</td>\n",
       "      <td>0.567949</td>\n",
       "      <td>0.685864</td>\n",
       "      <td>0.024217</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.027016</td>\n",
       "      <td>0.202218</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     num_transformer__popularity  num_transformer__duration_ms  \\\n",
       "8                       0.795699                      0.227306   \n",
       "865                     0.000000                      0.227711   \n",
       "390                     0.516129                      0.139312   \n",
       "618                     0.279570                      0.168962   \n",
       "836                     0.021505                      0.199739   \n",
       "..                           ...                           ...   \n",
       "431                     0.548387                      0.216160   \n",
       "12                      0.623656                      0.227552   \n",
       "191                     0.000000                      0.264154   \n",
       "312                     0.602151                      0.086616   \n",
       "231                     0.387097                      0.190209   \n",
       "\n",
       "     num_transformer__danceability  num_transformer__energy  \\\n",
       "8                         0.660256                 0.360435   \n",
       "865                       0.610256                 0.857304   \n",
       "390                       0.721795                 0.273175   \n",
       "618                       0.775641                 0.556514   \n",
       "836                       0.297436                 0.943538   \n",
       "..                             ...                      ...   \n",
       "431                       0.698718                 0.429217   \n",
       "12                        0.483333                 0.563700   \n",
       "191                       0.737179                 0.494918   \n",
       "312                       0.656410                 0.330664   \n",
       "231                       0.567949                 0.685864   \n",
       "\n",
       "     num_transformer__speechiness  num_transformer__instrumentalness  \\\n",
       "8                        0.015794                           0.000000   \n",
       "865                      0.124770                           0.000000   \n",
       "390                      0.029481                           0.000000   \n",
       "618                      0.055014                           0.000000   \n",
       "836                      0.145301                           0.000000   \n",
       "..                            ...                                ...   \n",
       "431                      0.021848                           0.000255   \n",
       "12                       0.011319                           0.000049   \n",
       "191                      0.072124                           0.000059   \n",
       "312                      0.042116                           0.000003   \n",
       "231                      0.024217                           0.000969   \n",
       "\n",
       "     num_transformer__liveness  num_transformer__tempo  \\\n",
       "8                     0.071724                0.303660   \n",
       "865                   0.298655                0.436410   \n",
       "390                   0.094184                0.462260   \n",
       "618                   0.089946                0.489139   \n",
       "836                   0.069075                0.621421   \n",
       "..                         ...                     ...   \n",
       "431                   0.100540                0.251327   \n",
       "12                    0.163047                0.195223   \n",
       "191                   0.065261                0.259910   \n",
       "312                   0.093124                0.686135   \n",
       "231                   0.027016                0.202218   \n",
       "\n",
       "     cat_transformer__explicit_False  cat_transformer__explicit_True  ...  \\\n",
       "8                                1.0                             0.0  ...   \n",
       "865                              1.0                             0.0  ...   \n",
       "390                              1.0                             0.0  ...   \n",
       "618                              1.0                             0.0  ...   \n",
       "836                              1.0                             0.0  ...   \n",
       "..                               ...                             ...  ...   \n",
       "431                              1.0                             0.0  ...   \n",
       "12                               1.0                             0.0  ...   \n",
       "191                              1.0                             0.0  ...   \n",
       "312                              1.0                             0.0  ...   \n",
       "231                              1.0                             0.0  ...   \n",
       "\n",
       "      us       wan    wan na      want       way      well      wont  \\\n",
       "8    0.0  0.000000  0.000000  0.351266  0.000000  0.000000  0.000000   \n",
       "865  0.0  0.000000  0.000000  0.073473  0.076609  0.333333  0.000000   \n",
       "390  0.0  0.000000  0.000000  0.000000  0.000000  0.247231  0.000000   \n",
       "618  0.0  0.000000  0.000000  0.456732  0.000000  0.000000  0.000000   \n",
       "836  0.0  0.000000  0.000000  0.243225  0.000000  0.000000  0.000000   \n",
       "..   ...       ...       ...       ...       ...       ...       ...   \n",
       "431  0.0  0.000000  0.000000  0.000000  0.125102  0.000000  0.229609   \n",
       "12   0.0  0.000000  0.000000  0.101838  0.000000  0.000000  0.000000   \n",
       "191  0.0  0.237914  0.241388  0.000000  0.000000  0.000000  0.000000   \n",
       "312  0.0  0.000000  0.000000  0.000000  0.435210  0.157803  0.000000   \n",
       "231  0.0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        world     would      yeah  \n",
       "8    0.000000  0.055069  0.000000  \n",
       "865  0.000000  0.000000  0.000000  \n",
       "390  0.278099  0.000000  0.000000  \n",
       "618  0.000000  0.000000  0.000000  \n",
       "836  0.000000  0.000000  0.000000  \n",
       "..        ...       ...       ...  \n",
       "431  0.000000  0.300954  0.066275  \n",
       "12   0.000000  0.000000  0.000000  \n",
       "191  0.000000  0.000000  0.000000  \n",
       "312  0.000000  0.000000  0.153707  \n",
       "231  0.000000  0.000000  0.000000  \n",
       "\n",
       "[800 rows x 78 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m embedding_size \u001b[39m=\u001b[39m \u001b[39m200\u001b[39m\n\u001b[0;32m----> 2\u001b[0m model_cnn \u001b[39m=\u001b[39m Sequential([\n\u001b[1;32m      3\u001b[0m     layers\u001b[39m.\u001b[39;49mEmbedding(input_dim\u001b[39m=\u001b[39;49mX_train\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], output_dim\u001b[39m=\u001b[39;49membedding_size, mask_zero\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m      4\u001b[0m     layers\u001b[39m.\u001b[39;49mConv1D(\u001b[39m20\u001b[39;49m, kernel_size\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m),\n\u001b[1;32m      5\u001b[0m     layers\u001b[39m.\u001b[39;49mFlatten(),\n\u001b[1;32m      6\u001b[0m     layers\u001b[39m.\u001b[39;49mDense(\u001b[39m1\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msigmoid\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m model_cnn\u001b[39m.\u001b[39msummary()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/tensorflow/python/trackable/base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/mood_detector/lib/python3.10/site-packages/keras/layers/core/dense.py:148\u001b[0m, in \u001b[0;36mDense.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    146\u001b[0m last_dim \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mdimension_value(input_shape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[39mif\u001b[39;00m last_dim \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe last dimension of the inputs to a Dense layer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshould be defined. Found None. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull input shape received: \u001b[39m\u001b[39m{\u001b[39;00minput_shape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_spec \u001b[39m=\u001b[39m InputSpec(min_ndim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axes\u001b[39m=\u001b[39m{\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m: last_dim})\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_weight(\n\u001b[1;32m    155\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mkernel\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    156\u001b[0m     shape\u001b[39m=\u001b[39m[last_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m     trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    162\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to a Dense layer should be defined. Found None. Full input shape received: (None, None)"
     ]
    }
   ],
   "source": [
    "embedding_size = 200\n",
    "model_cnn = Sequential([\n",
    "    layers.Embedding(input_dim=X_train.shape[1], output_dim=embedding_size, mask_zero=False),\n",
    "    layers.Conv1D(20, kernel_size=3),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='loss', patience=1)\n",
    "model_cnn.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_cnn.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=64, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mood_detector",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
